{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"acdc_data","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"tVMzz6edSpxb","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","import glob\n","import numpy as np\n","import logging\n","import nibabel as nib\n","import gc\n","import h5py\n","from skimage import transform\n","\n","import utils\n","import image_utils\n","\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n","\n","# Dictionary to translate a diagnosis into a number\n","# NOR  - Normal\n","# MINF - Previous myiocardial infarction (EF < 40%)\n","# DCM  - Dialated Cardiomypopathy\n","# HCM  - Hypertrophic cardiomyopathy\n","# RV   - Abnormal right ventricle (high volume or low EF)\n","diagnosis_dict = {'NOR': 0, 'MINF': 1, 'DCM': 2, 'HCM': 3, 'RV': 4}\n","\n","# Maximum number of data points that can be in memory at any time\n","MAX_WRITE_BUFFER = 5\n","\n","\n","def crop_or_pad_slice_to_size(slice, nx, ny):\n","\n","    x, y = slice.shape\n","\n","    x_s = (x - nx) // 2\n","    y_s = (y - ny) // 2\n","    x_c = (nx - x) // 2\n","    y_c = (ny - y) // 2\n","\n","    if x > nx and y > ny:\n","        slice_cropped = slice[x_s:x_s + nx, y_s:y_s + ny]\n","    else:\n","        slice_cropped = np.zeros((nx, ny))\n","        if x <= nx and y > ny:\n","            slice_cropped[x_c:x_c + x, :] = slice[:, y_s:y_s + ny]\n","        elif x > nx and y <= ny:\n","            slice_cropped[:, y_c:y_c + y] = slice[x_s:x_s + nx, :]\n","        else:\n","            slice_cropped[x_c:x_c + x, y_c:y_c + y] = slice[:, :]\n","\n","    return slice_cropped\n","\n","\n","def prepare_data(input_folder, output_file, mode, size, target_resolution, split_test_train=False):\n","\n","    '''\n","    Main function that prepares a dataset from the raw challenge data to an hdf5 dataset\n","    '''\n","\n","    assert (mode in ['2D', '3D']), 'Unknown mode: %s' % mode\n","    if mode == '2D' and not len(size) == 2:\n","        raise AssertionError('Inadequate number of size parameters')\n","    if mode == '3D' and not len(size) == 3:\n","        raise AssertionError('Inadequate number of size parameters')\n","    if mode == '2D' and not len(target_resolution) == 2:\n","        raise AssertionError('Inadequate number of target resolution parameters')\n","    if mode == '3D' and not len(target_resolution) == 3:\n","        raise AssertionError('Inadequate number of target resolution parameters')\n","\n","    hdf5_file = h5py.File(output_file, \"w\")\n","\n","    diag_list = {'test': [], 'train': []}\n","    height_list = {'test': [], 'train': []}\n","    weight_list = {'test': [], 'train': []}\n","    patient_id_list = {'test': [], 'train': []}\n","    cardiac_phase_list = {'test': [], 'train': []}\n","\n","    file_list = {'test': [], 'train': []}\n","    num_slices = {'test': 0, 'train': 0}\n","\n","    logging.info('Counting files and parsing meta data...')\n","\n","    for folder in os.listdir(input_folder):\n","\n","        folder_path = os.path.join(input_folder, folder)\n","\n","        if os.path.isdir(folder_path):\n","\n","            if split_test_train:\n","                train_test = 'test' if (int(folder[-3:]) % 5 == 0) else 'train'\n","            else:\n","                train_test = 'train'\n","\n","            infos = {}\n","            for line in open(os.path.join(folder_path, 'Info.cfg')):\n","                label, value = line.split(':')\n","                infos[label] = value.rstrip('\\n').lstrip(' ')\n","\n","            patient_id = folder.lstrip('patient')\n","\n","            for file in glob.glob(os.path.join(folder_path, 'patient???_frame??.nii.gz')):\n","\n","                file_list[train_test].append(file)\n","\n","                # diag_list[train_test].append(diagnosis_to_int(infos['Group']))\n","                diag_list[train_test].append(diagnosis_dict[infos['Group']])\n","                weight_list[train_test].append(infos['Weight'])\n","                height_list[train_test].append(infos['Height'])\n","\n","                patient_id_list[train_test].append(patient_id)\n","\n","                systole_frame = int(infos['ES'])\n","                diastole_frame = int(infos['ED'])\n","\n","                file_base = file.split('.')[0]\n","                frame = int(file_base.split('frame')[-1])\n","                if frame == systole_frame:\n","                    cardiac_phase_list[train_test].append(1)  # 1 == systole\n","                elif frame == diastole_frame:\n","                    cardiac_phase_list[train_test].append(2)  # 2 == diastole\n","                else:\n","                    cardiac_phase_list[train_test].append(0)  # 0 means other phase\n","\n","                nifty_img = nib.load(file)\n","                num_slices[train_test] += nifty_img.shape[2]\n","\n","\n","    # Write the small datasets\n","    for tt in ['test', 'train']:\n","        hdf5_file.create_dataset('diagnosis_%s' % tt, data=np.asarray(diag_list[tt], dtype=np.uint8))\n","        hdf5_file.create_dataset('weight_%s' % tt, data=np.asarray(weight_list[tt], dtype=np.float32))\n","        hdf5_file.create_dataset('height_%s' % tt, data=np.asarray(height_list[tt], dtype=np.float32))\n","        hdf5_file.create_dataset('patient_id_%s' % tt, data=np.asarray(patient_id_list[tt], dtype=np.uint8))\n","        hdf5_file.create_dataset('cardiac_phase_%s' % tt, data=np.asarray(cardiac_phase_list[tt], dtype=np.uint8))\n","\n","    if mode == '3D':\n","        nx, ny, nz_max = size\n","        n_train = len(file_list['train'])\n","        n_test = len(file_list['test'])\n","\n","    elif mode == '2D':\n","        nx, ny = size\n","        n_test = num_slices['test']\n","        n_train = num_slices['train']\n","\n","    else:\n","        raise AssertionError('Wrong mode setting. This should never happen.')\n","\n","    # Create datasets for images and masks\n","    data = {}\n","    for tt, num_points in zip(['test', 'train'], [n_test, n_train]):\n","\n","        if num_points > 0:\n","            data['images_%s' % tt] = hdf5_file.create_dataset(\"images_%s\" % tt, [num_points] + list(size), dtype=np.float32)\n","            data['masks_%s' % tt] = hdf5_file.create_dataset(\"masks_%s\" % tt, [num_points] + list(size), dtype=np.uint8)\n","\n","    mask_list = {'test': [], 'train': [] }\n","    img_list = {'test': [], 'train': [] }\n","\n","    logging.info('Parsing image files')\n","\n","    train_test_range = ['test', 'train'] if split_test_train else ['train']\n","    for train_test in train_test_range:\n","\n","        write_buffer = 0\n","        counter_from = 0\n","\n","        for file in file_list[train_test]:\n","\n","            logging.info('-----------------------------------------------------------')\n","            logging.info('Doing: %s' % file)\n","\n","            file_base = file.split('.nii.gz')[0]\n","            file_mask = file_base + '_gt.nii.gz'\n","\n","            img_dat = utils.load_nii(file)\n","            mask_dat = utils.load_nii(file_mask)\n","\n","            img = img_dat[0].copy()\n","            mask = mask_dat[0].copy()\n","\n","            img = image_utils.normalise_image(img)\n","\n","            pixel_size = (img_dat[2].structarr['pixdim'][1],\n","                          img_dat[2].structarr['pixdim'][2],\n","                          img_dat[2].structarr['pixdim'][3])\n","\n","            logging.info('Pixel size:')\n","            logging.info(pixel_size)\n","\n","            ### PROCESSING LOOP FOR 3D DATA ################################\n","            if mode == '3D':\n","\n","                scale_vector = [pixel_size[0] / target_resolution[0],\n","                                pixel_size[1] / target_resolution[1],\n","                                pixel_size[2]/ target_resolution[2]]\n","\n","                img_scaled = transform.rescale(img,\n","                                               scale_vector,\n","                                               order=1,\n","                                               preserve_range=True,\n","                                               multichannel=False,\n","                                               mode='constant')\n","                mask_scaled = transform.rescale(mask,\n","                                                scale_vector,\n","                                                order=0,\n","                                                preserve_range=True,\n","                                                multichannel=False,\n","                                                mode='constant')\n","\n","                slice_vol = np.zeros((nx, ny, nz_max), dtype=np.float32)\n","                mask_vol = np.zeros((nx, ny, nz_max), dtype=np.uint8)\n","\n","                nz_curr = img_scaled.shape[2]\n","                stack_from = (nz_max - nz_curr) // 2\n","\n","                if stack_from < 0:\n","                    raise AssertionError('nz_max is too small for the chosen through plane resolution. Consider changing'\n","                                         'the size or the target resolution in the through-plane.')\n","\n","                for zz in range(nz_curr):\n","\n","                    slice_rescaled = img_scaled[:,:,zz]\n","                    mask_rescaled = mask_scaled[:,:,zz]\n","\n","                    slice_cropped = crop_or_pad_slice_to_size(slice_rescaled, nx, ny)\n","                    mask_cropped = crop_or_pad_slice_to_size(mask_rescaled, nx, ny)\n","\n","                    slice_vol[:,:,stack_from] = slice_cropped\n","                    mask_vol[:,:,stack_from] = mask_cropped\n","\n","                    stack_from += 1\n","\n","                img_list[train_test].append(slice_vol)\n","                mask_list[train_test].append(mask_vol)\n","\n","                write_buffer += 1\n","\n","                if write_buffer >= MAX_WRITE_BUFFER:\n","\n","                    counter_to = counter_from + write_buffer\n","                    _write_range_to_hdf5(data, train_test, img_list, mask_list, counter_from, counter_to)\n","                    _release_tmp_memory(img_list, mask_list, train_test)\n","\n","                    # reset stuff for next iteration\n","                    counter_from = counter_to\n","                    write_buffer = 0\n","\n","            ### PROCESSING LOOP FOR SLICE-BY-SLICE 2D DATA ###################\n","            elif mode == '2D':\n","\n","                scale_vector = [pixel_size[0] / target_resolution[0], pixel_size[1] / target_resolution[1]]\n","\n","                for zz in range(img.shape[2]):\n","\n","                    slice_img = np.squeeze(img[:, :, zz])\n","                    slice_rescaled = transform.rescale(slice_img,\n","                                                       scale_vector,\n","                                                       order=1,\n","                                                       preserve_range=True,\n","                                                       multichannel=False,\n","                                                       mode = 'constant')\n","\n","                    slice_mask = np.squeeze(mask[:, :, zz])\n","                    mask_rescaled = transform.rescale(slice_mask,\n","                                                      scale_vector,\n","                                                      order=0,\n","                                                      preserve_range=True,\n","                                                      multichannel=False,\n","                                                      mode='constant')\n","\n","                    slice_cropped = crop_or_pad_slice_to_size(slice_rescaled, nx, ny)\n","                    mask_cropped = crop_or_pad_slice_to_size(mask_rescaled, nx, ny)\n","\n","                    img_list[train_test].append(slice_cropped)\n","                    mask_list[train_test].append(mask_cropped)\n","\n","                    write_buffer += 1\n","\n","                    # Writing needs to happen inside the loop over the slices\n","                    if write_buffer >= MAX_WRITE_BUFFER:\n","\n","                        counter_to = counter_from + write_buffer\n","                        _write_range_to_hdf5(data, train_test, img_list, mask_list, counter_from, counter_to)\n","                        _release_tmp_memory(img_list, mask_list, train_test)\n","\n","                        # reset stuff for next iteration\n","                        counter_from = counter_to\n","                        write_buffer = 0\n","\n","        # after file loop: Write the remaining data\n","\n","        logging.info('Writing remaining data')\n","        counter_to = counter_from + write_buffer\n","\n","        _write_range_to_hdf5(data, train_test, img_list, mask_list, counter_from, counter_to)\n","        _release_tmp_memory(img_list, mask_list, train_test)\n","\n","\n","    # After test train loop:\n","    hdf5_file.close()\n","\n","\n","def _write_range_to_hdf5(hdf5_data, train_test, img_list, mask_list, counter_from, counter_to):\n","    '''\n","    Helper function to write a range of data to the hdf5 datasets\n","    '''\n","\n","    logging.info('Writing data from %d to %d' % (counter_from, counter_to))\n","\n","    img_arr = np.asarray(img_list[train_test], dtype=np.float32)\n","    mask_arr = np.asarray(mask_list[train_test], dtype=np.uint8)\n","\n","    hdf5_data['images_%s' % train_test][counter_from:counter_to, ...] = img_arr\n","    hdf5_data['masks_%s' % train_test][counter_from:counter_to, ...] = mask_arr\n","\n","\n","def _release_tmp_memory(img_list, mask_list, train_test):\n","    '''\n","    Helper function to reset the tmp lists and free the memory\n","    '''\n","\n","    img_list[train_test].clear()\n","    mask_list[train_test].clear()\n","    gc.collect()\n","\n","\n","def load_and_maybe_process_data(input_folder,\n","                                preprocessing_folder,\n","                                mode,\n","                                size,\n","                                target_resolution,\n","                                force_overwrite=False,\n","                                split_test_train=False):\n","\n","    '''\n","    This function is used to load and if necessary preprocesses the ACDC challenge data\n","    \n","    :param input_folder: Folder where the raw ACDC challenge data is located \n","    :param preprocessing_folder: Folder where the proprocessed data should be written to\n","    :param mode: Can either be '2D' or '3D'. 2D saves the data slice-by-slice, 3D saves entire volumes\n","    :param size: Size of the output slices/volumes in pixels/voxels\n","    :param target_resolution: Resolution to which the data should resampled. Should have same shape as size\n","    :param force_overwrite: Set this to True if you want to overwrite already preprocessed data [default: False]\n","     \n","    :return: Returns an h5py.File handle to the dataset\n","    '''\n","\n","    size_str = '_'.join([str(i) for i in size])\n","    res_str = '_'.join([str(i) for i in target_resolution])\n","\n","    if not split_test_train:\n","        data_file_name = 'data_%s_size_%s_res_%s_onlytrain.hdf5' % (mode, size_str, res_str)\n","    else:\n","        data_file_name = 'data_%s_size_%s_res_%s.hdf5' % (mode, size_str, res_str)\n","\n","    data_file_path = os.path.join(preprocessing_folder, data_file_name)\n","\n","    utils.makefolder(preprocessing_folder)\n","\n","    if not os.path.exists(data_file_path) or force_overwrite:\n","\n","        logging.info('This configuration of mode, size and target resolution has not yet been preprocessed')\n","        logging.info('Preprocessing now!')\n","        prepare_data(input_folder, data_file_path, mode, size, target_resolution, split_test_train=split_test_train)\n","\n","    elif os.path.getsize(data_file_path) < 10485760:  # If file is smaller than 10MB\n","\n","        logging.warning('WARNING: Your preprocessed data file is smaller than 10MB. It is likely that something went '\n","                        'wrong with the preprocessing.')\n","        logging.warning(\"To make sure, delete '%s' and run the code again\")\n","        logging.info('Continuing anyway...')\n","\n","    else:\n","\n","        logging.info('Already preprocessed this configuration. Loading now!')\n","\n","    return h5py.File(data_file_path, 'r')\n","\n","\n","if __name__ == '__main__':\n","\n","    input_folder = '/content/drive/My Drive/ACDC_challenge/train'\n","    preprocessing_folder = '/content/drive/My Drive/preproc_data'\n","\n","    # d=load_and_maybe_process_data(input_folder, preprocessing_folder, '3D', (116,116,28), (2.5,2.5,5))\n","    d=load_and_maybe_process_data(input_folder, preprocessing_folder, '2D', (212,212), (1.36719, 1.36719))"],"execution_count":0,"outputs":[]}]}